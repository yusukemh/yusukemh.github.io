---
layout: essay
type: essay
title: Perfection, or Quit.
# All dates must be YYYY-MM-DD format!
date: 2019-04-17
labels:
  - Artificial Intelligence
  - Ethic
---

0 or 100. This, I believe, is the basic and the most fundamental principle when it comes to "automated" devices. 

Our life is becoming more and more convenient and efficient thanks to devices that can do jobs and decision making for human beings. Some involve simple programing while others have to deal with complex situations thus having to implement highly developed artificial intelligence. One example of the latter would be automated car driving system, which can instantly become controversy involving ethical consideration. 

[This article](https://www.technologyreview.com/s/542626/why-self-driving-cars-must-be-programmed-to-kill/) gives you brief idea why automated driving systems have to deal with ethical issue. Briefly speaking, there are some situations where no sacrifice can be avoided on the road, and it becomes utterly complicated how the system has to be programmed to make decisions. For example, if there is a group of people walking on red in front of a vehicle, should the car keep going to kill them, or swerve to the side, which results in the driver's death even though it was totally pedestrians' fault for crossing on red? What if the driver is rushing to the hospital? What if there is another person on the side and the person has to be killed as a result of the car's swerving to the side? What if the crossing light was broken, thus pedestrians are actually NOT crossing on the red, but green? 

Question can arise non-stop. The situation can be much complex with practically infinite number of possibilities. So, does that mean manufacturers have to at some point decide how the system makes decision, involving ethical choice? Then, does that mean they have to be legally accused of programming so, or not?

The simplest answer is: stop programming if it is not perfect. Yes, it is true that automated systems can help us to a great extent, but there is one fundamental border; they are just helping us, not taking control from us. As soon as they take over full control from us, that's when the system has to be perfect. Otherwise, humans are the final decision makers. Instead of trying to figure out how to deal with infinitely complex situations, make it humans' decision. Or, try to avoid getting into unavoidable situation, which is impossible. Drivers have to be aware that any consequence is in their hand, and automated system can quit supporting you at any point.

In the end, humans are the final decision maker. No matter what, they should not be separated from the obligation for the safety and the responsibility of consequences. Ethical decision making is way beyond automated system's capability. 
